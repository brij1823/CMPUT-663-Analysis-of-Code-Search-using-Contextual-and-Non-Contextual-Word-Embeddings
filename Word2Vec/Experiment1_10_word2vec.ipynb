{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e999d970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Python Libraries\n",
    "import ast\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import astor\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# #pip install nmslib\n",
    "#import nmslib\n",
    "import numpy\n",
    "import re\n",
    "#python -m spacy download en_core_web_sm\n",
    "EN = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c732cf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading the dataset\n",
    "# ! wget https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/python.zip\n",
    "# ! unzip python.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dc82b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:14, 696.42it/s]\n",
      "/var/folders/b0/nlrr4t_x3j5_jvwwktffc3j00000gn/T/ipykernel_52588/323944334.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_trn_sub['code token'] = codeToken\n",
      "/var/folders/b0/nlrr4t_x3j5_jvwwktffc3j00000gn/T/ipykernel_52588/323944334.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_trn_sub['doc token'] = docToken\n",
      "/var/folders/b0/nlrr4t_x3j5_jvwwktffc3j00000gn/T/ipykernel_52588/323944334.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_trn_sub['clean_code'] = clean_code_token\n",
      "400it [00:02, 135.28it/s]\n",
      "100%|███████████████████████████████████████| 400/400 [2:51:53<00:00, 25.78s/it]\n"
     ]
    }
   ],
   "source": [
    "#Reading files from databases\n",
    "path = Path('.')\n",
    "def jsonl_list_to_dataframe(file_list, columns=None):\n",
    "    \"\"\"Load a list of jsonl.gz files into a pandas DataFrame.\"\"\"\n",
    "    return pd.concat([pd.read_json(f,\n",
    "                                   orient='records', \n",
    "                                   compression='gzip',\n",
    "                                   lines=True)[columns] \n",
    "                      for f in file_list], sort=False)\n",
    "\n",
    "def get_dfs(path):\n",
    "    \"\"\"Grabs the different data splits and converts them into dataframes\"\"\"\n",
    "    dfs = []\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        files = sorted((path/split).glob(\"**/*.gz\"))\n",
    "        df = jsonl_list_to_dataframe(files, [\"code\", \"docstring\",\"code_tokens\",\"docstring_tokens\",\"url\"])\n",
    "        dfs.append(df)\n",
    "        \n",
    "    return dfs\n",
    "\n",
    "\n",
    "def tokenize_docstring(text):\n",
    "    \"Apply tokenization using spacy to docstrings.\"\n",
    "    tokens = EN.tokenizer(text)\n",
    "    return [token.text.lower() for token in tokens if not token.is_space]\n",
    "\n",
    "\n",
    "def tokenize_code(text):\n",
    "    \"A very basic procedure for tokenizing code strings.\"\n",
    "    return RegexpTokenizer(r'\\w+').tokenize(text)\n",
    "\n",
    "df_trn, df_val, df_tst = get_dfs(path/\"python/final/jsonl\")\n",
    "\n",
    "#Subsample\n",
    "df_trn_sub = df_trn[:10000]\n",
    "print(df_trn_sub.shape)\n",
    "docstring = list(df_trn_sub['docstring'].values)\n",
    "code = list(df_trn_sub['code'].values)\n",
    "#print(code[0])\n",
    "codeToken = []\n",
    "docToken = []\n",
    "\n",
    "for singleCode,singleDoc in tqdm(zip(code,docstring)):\n",
    "  codeToken.append(' '.join(tokenize_code(singleCode)))\n",
    "  docToken.append(' '.join(tokenize_docstring(singleDoc)))\n",
    "df_trn_sub['code token'] = codeToken\n",
    "df_trn_sub['doc token'] = docToken\n",
    "#Clean Code Token\n",
    "code_token = list(df_trn_sub['code_tokens'].values)\n",
    "code_token_list = []\n",
    "for code in code_token:\n",
    "  code_token_list.append(' '.join(code))\n",
    "\n",
    "import re\n",
    "def pre_process(text):\n",
    "    \n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text=re.sub(\"\",\"\",text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "df_idf = pd.DataFrame()\n",
    "df_idf['text'] = code_token_list\n",
    "df_idf['text'] = df_idf['text'].apply(lambda x:pre_process(x))\n",
    "clean_code = list(df_idf['text'].values)\n",
    "clean_code_token = []\n",
    "for code in clean_code:\n",
    "  clean_code_token.append(code.split(' '))\n",
    "df_trn_sub['clean_code'] = clean_code_token\n",
    "\n",
    "\n",
    "#Original Tokens\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer=CountVectorizer()\n",
    "data_corpus = list(df_trn_sub.clean_code.values)\n",
    "document_corpus = list(df_trn_sub.docstring.values)\n",
    "buffer = []\n",
    "for data in data_corpus:buffer.append(' '.join(data))\n",
    "vocabulary=vectorizer.fit(buffer)\n",
    "X= vectorizer.transform(buffer)\n",
    "# print('Code Tokens : ',vocabulary.get_feature_names())\n",
    "#print('Word Embeddings : ',X.toarray())\n",
    "baseData = X.toarray()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import ndcg_score\n",
    "from tqdm import tqdm\n",
    "testDocstring = list(df_tst.docstring.values)\n",
    "filteredDocstring = []\n",
    "for doc in testDocstring:\n",
    "    buffer = doc.split(' ')\n",
    "    if len(buffer) <10 :filteredDocstring.append(doc)\n",
    "    #filteredDocstring.append(doc)\n",
    "    \n",
    "filteredDocstring = filteredDocstring[:400]\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "closest_doc_indexes = {}\n",
    "testingVector = vectorizer.transform(filteredDocstring)\n",
    "doctrainingVector = vectorizer.transform(document_corpus)\n",
    "for index, test_doc_embd in tqdm(enumerate(testingVector)):\n",
    "  test_doc_embd = np.reshape(test_doc_embd,(1,-1))\n",
    "  similarities = cosine_similarity(test_doc_embd,doctrainingVector)[0]\n",
    "  \n",
    "  most_similar_train_doc = np.argmax(similarities)\n",
    "  closest_doc_indexes[index] = most_similar_train_doc\n",
    "\n",
    "#print(closest_doc_indexes)\n",
    "\n",
    "\n",
    "result = []\n",
    "ndcga = []\n",
    "mrr = []\n",
    "for doc in tqdm(filteredDocstring):\n",
    "    testingString = [doc]\n",
    "    testingVector = vectorizer.transform(testingString)\n",
    "    testingVector = testingVector.toarray()\n",
    "    distances = distance.cdist(testingVector, baseData, \"cosine\")[0]\n",
    "    min_index = np.argmin(distances)\n",
    "    min_distance = distances[min_index]\n",
    "    max_similarity = 1 - min_distance\n",
    "    result.append(max_similarity)\n",
    "    ndcga.append((ndcg_score(np.asarray(testingVector), np.asarray([baseData[min_index]]))))\n",
    "    index = filteredDocstring.index(doc)\n",
    "    closest = closest_doc_indexes[index]\n",
    "    best = np.argsort(distances)[:50]\n",
    "    \n",
    "    if closest in best:\n",
    "      best = list(best)\n",
    "      mrr.append(1/(best.index(closest)+1))\n",
    "    else:\n",
    "      mrr.append(0)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96f36e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5273037586640298\n",
      "0.38574048077399675\n",
      "0.031637787403105605\n"
     ]
    }
   ],
   "source": [
    "result = [x for x in result if str(x) != 'nan']\n",
    "\n",
    "print('NDCGA Results : ',sum(ndcga)/len(ndcga))\n",
    "print('Similarity Average : ',sum(result)/len(result))\n",
    "print('Mean Reciprocal Rank :  ',sum(mrr)/len(mrr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb59200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
