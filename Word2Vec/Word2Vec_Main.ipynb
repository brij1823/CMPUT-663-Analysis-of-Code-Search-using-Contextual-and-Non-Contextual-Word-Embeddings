{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec Main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Drive to Colab"
      ],
      "metadata": {
        "id": "J-Qb7KEgILAe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nit1gI2QniQ",
        "outputId": "56aa9e41-f462-47ef-cae5-e2180ff33a4b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zx7xAU5-kwuG",
        "outputId": "db7d4430-d8ec-45ba-9caf-bb2439bd4c4f"
      },
      "source": [
        "cd /content/drive/Shareddrives/663-SoftwareAnalytics/Codebase/Brij_word2vec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shareddrives/663-SoftwareAnalytics/Codebase/Brij_word2vec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Python Library"
      ],
      "metadata": {
        "id": "8RVnCJizISpF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RiCFxpelYic"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import ast\n",
        "import glob\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "import astor\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "EN = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Parsing from database"
      ],
      "metadata": {
        "id": "sO5nFRmPIWok"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "vdOIOyLjkzSp",
        "outputId": "cba66d6d-b9d2-4ad7-f151-a64f8b54aebe"
      },
      "source": [
        "\n",
        "#hide\n",
        "\n",
        "path = Path('.')\n",
        "#collapse_show\n",
        "def jsonl_list_to_dataframe(file_list, columns=None):\n",
        "    \"\"\"Load a list of jsonl.gz files into a pandas DataFrame.\"\"\"\n",
        "    return pd.concat([pd.read_json(f,\n",
        "                                   orient='records', \n",
        "                                   compression='gzip',\n",
        "                                   lines=True)[columns] \n",
        "                      for f in file_list], sort=False)\n",
        "\n",
        "def get_dfs(path):\n",
        "    \"\"\"Grabs the different data splits and converts them into dataframes\"\"\"\n",
        "    dfs = []\n",
        "    for split in [\"train\", \"valid\", \"test\"]:\n",
        "        files = sorted((path/split).glob(\"**/*.gz\"))\n",
        "        df = jsonl_list_to_dataframe(files, [\"code\", \"docstring\",\"code_tokens\",\"docstring_tokens\",\"url\"])\n",
        "        dfs.append(df)\n",
        "        \n",
        "    return dfs\n",
        "\n",
        "\n",
        "def tokenize_docstring(text):\n",
        "    \"Apply tokenization using spacy to docstrings.\"\n",
        "    tokens = EN.tokenizer(text)\n",
        "    return [token.text.lower() for token in tokens if not token.is_space]\n",
        "\n",
        "\n",
        "def tokenize_code(text):\n",
        "    \"A very basic procedure for tokenizing code strings.\"\n",
        "    return RegexpTokenizer(r'\\w+').tokenize(text)\n",
        "\n",
        "df_trn, df_val, df_tst = get_dfs(path/\"python/final/jsonl\")\n",
        "df_trn.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code</th>\n",
              "      <th>docstring</th>\n",
              "      <th>code_tokens</th>\n",
              "      <th>docstring_tokens</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>def train(train_dir, model_save_path=None, n_n...</td>\n",
              "      <td>Trains a k-nearest neighbors classifier for fa...</td>\n",
              "      <td>[def, train, (, train_dir, ,, model_save_path,...</td>\n",
              "      <td>[Trains, a, k, -, nearest, neighbors, classifi...</td>\n",
              "      <td>https://github.com/ageitgey/face_recognition/b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>def predict(X_img_path, knn_clf=None, model_pa...</td>\n",
              "      <td>Recognizes faces in given image using a traine...</td>\n",
              "      <td>[def, predict, (, X_img_path, ,, knn_clf, =, N...</td>\n",
              "      <td>[Recognizes, faces, in, given, image, using, a...</td>\n",
              "      <td>https://github.com/ageitgey/face_recognition/b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>def show_prediction_labels_on_image(img_path, ...</td>\n",
              "      <td>Shows the face recognition results visually.\\n...</td>\n",
              "      <td>[def, show_prediction_labels_on_image, (, img_...</td>\n",
              "      <td>[Shows, the, face, recognition, results, visua...</td>\n",
              "      <td>https://github.com/ageitgey/face_recognition/b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>def _rect_to_css(rect):\\n    \"\"\"\\n    Convert ...</td>\n",
              "      <td>Convert a dlib 'rect' object to a plain tuple ...</td>\n",
              "      <td>[def, _rect_to_css, (, rect, ), :, return, rec...</td>\n",
              "      <td>[Convert, a, dlib, rect, object, to, a, plain,...</td>\n",
              "      <td>https://github.com/ageitgey/face_recognition/b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>def _trim_css_to_bounds(css, image_shape):\\n  ...</td>\n",
              "      <td>Make sure a tuple in (top, right, bottom, left...</td>\n",
              "      <td>[def, _trim_css_to_bounds, (, css, ,, image_sh...</td>\n",
              "      <td>[Make, sure, a, tuple, in, (, top, right, bott...</td>\n",
              "      <td>https://github.com/ageitgey/face_recognition/b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                code  ...                                                url\n",
              "0  def train(train_dir, model_save_path=None, n_n...  ...  https://github.com/ageitgey/face_recognition/b...\n",
              "1  def predict(X_img_path, knn_clf=None, model_pa...  ...  https://github.com/ageitgey/face_recognition/b...\n",
              "2  def show_prediction_labels_on_image(img_path, ...  ...  https://github.com/ageitgey/face_recognition/b...\n",
              "3  def _rect_to_css(rect):\\n    \"\"\"\\n    Convert ...  ...  https://github.com/ageitgey/face_recognition/b...\n",
              "4  def _trim_css_to_bounds(css, image_shape):\\n  ...  ...  https://github.com/ageitgey/face_recognition/b...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sumsampling the database and generate word2vec Embedding"
      ],
      "metadata": {
        "id": "HXf1ZWVjIjYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Rq6ry2BoIjXD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5t_UrSalAcr",
        "outputId": "107ce1fd-bbab-4631-8503-8e995f4a6157"
      },
      "source": [
        "#Subsample\n",
        "df_trn_sub = df_trn[:5000]\n",
        "docstring = list(df_trn_sub['docstring'].values)\n",
        "code = list(df_trn_sub['code'].values)\n",
        "codeToken = []\n",
        "docToken = []\n",
        "\n",
        "for singleCode,singleDoc in tqdm(zip(code,docstring)):\n",
        "  codeToken.append(' '.join(tokenize_code(singleCode)))\n",
        "  docToken.append(' '.join(tokenize_docstring(singleDoc)))\n",
        "df_trn_sub['code token'] = codeToken\n",
        "df_trn_sub['doc token'] = docToken\n",
        "code_token = list(df_trn_sub['code_tokens'].values)\n",
        "\n",
        "code_token_list = []\n",
        "for code in code_token:\n",
        "  code_token_list.append(' '.join(code))\n",
        "\n",
        "import re\n",
        "def pre_process(text):\n",
        "    \n",
        "    # lowercase\n",
        "    text=text.lower()\n",
        "    \n",
        "    #remove tags\n",
        "    text=re.sub(\"\",\"\",text)\n",
        "    \n",
        "    # remove special characters and digits\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "#Cleaning the code tokens\n",
        "df_idf = pd.DataFrame()\n",
        "df_idf['text'] = code_token_list\n",
        "df_idf['text'] = df_idf['text'].apply(lambda x:pre_process(x))\n",
        "clean_code = list(df_idf['text'].values)\n",
        "clean_code_token = []\n",
        "for code in clean_code:\n",
        "  clean_code_token.append(code.split(' '))\n",
        "df_trn_sub['clean_code'] = clean_code_token\n",
        "\n",
        "#Generate the code and doc embeddings\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer=CountVectorizer()\n",
        "data_corpus = list(df_trn_sub.clean_code.values)\n",
        "document_corpus = list(df_trn_sub.docstring.values)\n",
        "buffer = []\n",
        "for data in data_corpus:buffer.append(' '.join(data[:200]))\n",
        "vocabulary=vectorizer.fit(buffer)\n",
        "X= vectorizer.transform(buffer)\n",
        "baseData = X.toarray()\n",
        "all_embeddings = np.array(baseData)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5000it [00:03, 1534.47it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V30X0sgdt5qB"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "from sklearn.metrics import ndcg_score\n",
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "\n",
        "testDocstring = list(df_tst.docstring.values)\n",
        "filteredDocstring = []\n",
        "for doc in testDocstring:\n",
        "  buffer = doc.split(' ')\n",
        "  #if len(buffer) < 10:filteredDocstring.append(doc)\n",
        "  filteredDocstring.append(doc)\n",
        "filteredDocstring = filteredDocstring[:400]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3q9PfyyS3O7",
        "outputId": "872d1f36-790c-4b30-dc40-b7d1d59e368e"
      },
      "source": [
        "print(filteredDocstring[:5],len(filteredDocstring))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Extracts video ID from URL.', 'str->list\\n    Convert XML to URL List.\\n    From Biligrab.', 'From http://cdn37.atwikiimg.com/sitescript/pub/dksitescript/FC2.site.js\\n    Also com.hps.util.fc2.FC2EncrptUtil.makeMimiLocal\\n    L110', 'wrapper', 'Downloads Dailymotion videos by URL.'] 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9hjiwiie0JD",
        "outputId": "2d482ce7-db00-4472-e0a0-4e9a407e5b51"
      },
      "source": [
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "closest_doc_indexes = {}\n",
        "testingVector = vectorizer.transform(filteredDocstring)\n",
        "doctrainingVector = vectorizer.transform(document_corpus)\n",
        "for index, test_doc_embd in tqdm(enumerate(testingVector)):\n",
        "  test_doc_embd = np.reshape(test_doc_embd,(1,-1))\n",
        "  similarities = cosine_similarity(test_doc_embd,doctrainingVector)[0]\n",
        "  \n",
        "  most_similar_train_doc = np.argmax(similarities)\n",
        "  closest_doc_indexes[index] = most_similar_train_doc\n",
        "\n",
        "print(closest_doc_indexes)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "400it [00:01, 229.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 3683, 1: 3992, 2: 4205, 3: 4198, 4: 4445, 5: 3286, 6: 3731, 7: 3731, 8: 4398, 9: 4398, 10: 4445, 11: 4198, 12: 3979, 13: 2107, 14: 1750, 15: 3683, 16: 3971, 17: 3959, 18: 1734, 19: 3765, 20: 2372, 21: 2107, 22: 3942, 23: 3868, 24: 3400, 25: 4293, 26: 2659, 27: 3683, 28: 4293, 29: 737, 30: 440, 31: 3183, 32: 4445, 33: 710, 34: 3816, 35: 3327, 36: 2140, 37: 4741, 38: 3343, 39: 1137, 40: 4574, 41: 1999, 42: 2394, 43: 2394, 44: 1065, 45: 1131, 46: 3942, 47: 2107, 48: 3805, 49: 3099, 50: 3805, 51: 3031, 52: 2243, 53: 1953, 54: 2351, 55: 4574, 56: 3782, 57: 3592, 58: 3031, 59: 636, 60: 703, 61: 715, 62: 3282, 63: 632, 64: 366, 65: 1717, 66: 3782, 67: 703, 68: 3572, 69: 664, 70: 3543, 71: 3513, 72: 4864, 73: 3259, 74: 3259, 75: 3259, 76: 3820, 77: 3587, 78: 1750, 79: 3357, 80: 673, 81: 418, 82: 407, 83: 673, 84: 158, 85: 673, 86: 673, 87: 673, 88: 407, 89: 972, 90: 3282, 91: 3285, 92: 3441, 93: 695, 94: 1842, 95: 1842, 96: 3285, 97: 156, 98: 3466, 99: 3296, 100: 3751, 101: 3744, 102: 3282, 103: 3691, 104: 4864, 105: 3256, 106: 2819, 107: 3031, 108: 4710, 109: 501, 110: 1717, 111: 4447, 112: 660, 113: 3132, 114: 3375, 115: 3952, 116: 24, 117: 3768, 118: 3768, 119: 3236, 120: 3031, 121: 3688, 122: 3704, 123: 3959, 124: 3189, 125: 3150, 126: 3758, 127: 4992, 128: 1717, 129: 2508, 130: 3282, 131: 3782, 132: 3744, 133: 2760, 134: 3594, 135: 3959, 136: 429, 137: 1725, 138: 2861, 139: 2422, 140: 2861, 141: 3588, 142: 1823, 143: 3691, 144: 1823, 145: 536, 146: 2405, 147: 2117, 148: 3141, 149: 326, 150: 2377, 151: 3256, 152: 972, 153: 432, 154: 2377, 155: 972, 156: 3282, 157: 632, 158: 632, 159: 3505, 160: 632, 161: 972, 162: 3542, 163: 3543, 164: 632, 165: 672, 166: 3816, 167: 912, 168: 1604, 169: 3782, 170: 2433, 171: 3959, 172: 3589, 173: 4067, 174: 4447, 175: 488, 176: 524, 177: 2914, 178: 2752, 179: 2452, 180: 1838, 181: 3282, 182: 2920, 183: 3282, 184: 3679, 185: 2785, 186: 1283, 187: 1764, 188: 3860, 189: 3507, 190: 3664, 191: 432, 192: 432, 193: 432, 194: 4190, 195: 3504, 196: 3820, 197: 3360, 198: 2102, 199: 2586, 200: 4073, 201: 3282, 202: 4557, 203: 1750, 204: 3680, 205: 3747, 206: 1750, 207: 3747, 208: 1155, 209: 3680, 210: 3747, 211: 3747, 212: 1155, 213: 3952, 214: 4864, 215: 4201, 216: 673, 217: 3751, 218: 3038, 219: 1455, 220: 1481, 221: 3727, 222: 4888, 223: 917, 224: 1523, 225: 900, 226: 3902, 227: 1155, 228: 972, 229: 3542, 230: 1155, 231: 3638, 232: 3960, 233: 3960, 234: 3747, 235: 972, 236: 432, 237: 1734, 238: 3707, 239: 3318, 240: 638, 241: 432, 242: 1717, 243: 501, 244: 3571, 245: 277, 246: 156, 247: 1763, 248: 972, 249: 3038, 250: 3390, 251: 4190, 252: 3255, 253: 3337, 254: 1851, 255: 3554, 256: 673, 257: 501, 258: 3282, 259: 3782, 260: 3176, 261: 1717, 262: 169, 263: 940, 264: 3820, 265: 2756, 266: 1435, 267: 632, 268: 3782, 269: 1717, 270: 1996, 271: 219, 272: 3441, 273: 900, 274: 155, 275: 4013, 276: 4992, 277: 1512, 278: 3271, 279: 2674, 280: 3282, 281: 4778, 282: 695, 283: 1384, 284: 1548, 285: 3782, 286: 637, 287: 1819, 288: 3513, 289: 1136, 290: 326, 291: 3952, 292: 4519, 293: 3031, 294: 3031, 295: 34, 296: 4992, 297: 1001, 298: 279, 299: 3820, 300: 3960, 301: 1995, 302: 3960, 303: 3960, 304: 292, 305: 277, 306: 3193, 307: 1734, 308: 158, 309: 972, 310: 3598, 311: 4864, 312: 1571, 313: 1571, 314: 219, 315: 1717, 316: 2687, 317: 488, 318: 3959, 319: 3820, 320: 900, 321: 3505, 322: 3505, 323: 3577, 324: 3747, 325: 689, 326: 3542, 327: 1591, 328: 3959, 329: 223, 330: 3820, 331: 3316, 332: 4988, 333: 2242, 334: 432, 335: 4977, 336: 637, 337: 1169, 338: 3884, 339: 3282, 340: 4659, 341: 2390, 342: 446, 343: 74, 344: 3152, 345: 3152, 346: 3147, 347: 3820, 348: 3256, 349: 4157, 350: 1943, 351: 1577, 352: 3031, 353: 972, 354: 1283, 355: 3624, 356: 3577, 357: 1829, 358: 3617, 359: 1717, 360: 488, 361: 632, 362: 3193, 363: 3282, 364: 673, 365: 3831, 366: 668, 367: 4455, 368: 3028, 369: 673, 370: 673, 371: 673, 372: 1717, 373: 673, 374: 876, 375: 972, 376: 972, 377: 972, 378: 3747, 379: 3852, 380: 3747, 381: 3126, 382: 488, 383: 3126, 384: 3816, 385: 488, 386: 972, 387: 3554, 388: 3282, 389: 3282, 390: 3126, 391: 3031, 392: 4440, 393: 3760, 394: 4447, 395: 4447, 396: 3624, 397: 3624, 398: 2279, 399: 3679}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uus4PBn1i3OC"
      },
      "source": [
        "import pickle\n",
        "with open('saved_dictionary.pkl', 'wb') as f:\n",
        "    pickle.dump(closest_doc_indexes, f)\n",
        "        \n",
        "with open('saved_dictionary.pkl', 'rb') as f:\n",
        "        loaded_dict = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-XIW9GuUtM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5af22df-6141-4ada-cfe2-ec6ae1e62a9b"
      },
      "source": [
        "\n",
        "from tqdm import tqdm\n",
        "result = []\n",
        "ndcga = []\n",
        "mrr = []\n",
        "for doc in tqdm(filteredDocstring):\n",
        "    testingString = [doc]\n",
        "    testingVector = vectorizer.transform(testingString)\n",
        "    testingVector = testingVector.toarray()\n",
        "    distances = distance.cdist(testingVector, baseData, \"cosine\")[0]\n",
        "    min_index = np.argmin(distances)\n",
        "    min_distance = distances[min_index]\n",
        "    max_similarity = 1 - min_distance\n",
        "    result.append(max_similarity)\n",
        "    ndcga.append((ndcg_score(np.asarray(testingVector), np.asarray([baseData[min_index]]))))\n",
        "    index = filteredDocstring.index(doc)\n",
        "    closest = closest_doc_indexes[index]\n",
        "    best = np.argsort(distances)[:50]\n",
        "    if closest in best:\n",
        "      best = list(best)\n",
        "      mrr.append(1/(best.index(closest)+1))\n",
        "    else:\n",
        "      mrr.append(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400/400 [02:44<00:00,  2.43it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdjX5f1eS65I",
        "outputId": "be6c731b-0b71-4c75-d5eb-498028942094"
      },
      "source": [
        "result = [x for x in result if str(x) != 'nan']\n",
        "\n",
        "print('NDCGA Results : ',sum(ndcga)/len(ndcga))\n",
        "print('Similarity Average : ',sum(result)/len(result))\n",
        "print('Mean Reciprocal Rank :  ',sum(mrr)/len(mrr))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5448739729695309\n",
            "0.40063446923237145\n",
            "0.0220853657129706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdi0x1HduXt9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}