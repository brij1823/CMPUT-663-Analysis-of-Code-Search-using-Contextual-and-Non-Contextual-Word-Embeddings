{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generate_CodeBERT_embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWFu0YmGIwqL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cdaa3dc-53f8-49d4-8fa4-74c0943b5a55"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install nmslib\n",
        "!pip install tensorboardX\n",
        "!pip install transformers\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import json\n",
        "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
        "import glob\n",
        "import nmslib\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: nmslib in /usr/local/lib/python3.7/dist-packages (2.1.1)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from nmslib) (1.19.5)\n",
            "Requirement already satisfied: pybind11<2.6.2 in /usr/local/lib/python3.7/dist-packages (from nmslib) (2.6.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from nmslib) (5.4.8)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.13.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Code and Docstring embeddings "
      ],
      "metadata": {
        "id": "DqWsa4ggXCqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings for Training data"
      ],
      "metadata": {
        "id": "pDDIYku9XLjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings for Docstrings"
      ],
      "metadata": {
        "id": "M-IIoXBGXrao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracted and formatted dataset can be downloaded from this [drive link](https://drive.google.com/file/d/1DBSW_R-vXMXewkv5WgfzPBF8vhoSKrwg/view?usp=sharing)"
      ],
      "metadata": {
        "id": "0RN6l-9x1eUk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeVjuTGFpeV7"
      },
      "source": [
        "# extract the downloaded data to the path and change the following paths  \n",
        "train_path = './data/jsonl/train/*.jsonl'\n",
        "test_path =  './data/jsonl/test/python_test_0.jsonl'"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Csz-I_i3LPoI"
      },
      "source": [
        "# this code generates embeddings for training docstrings \n",
        "\n",
        "def find_embs(start,end):\n",
        "\n",
        "  trian_files = glob.glob(train_path)\n",
        "  train_data_CLEARTHIS = []\n",
        "  for each_file in trian_files:\n",
        "    with open(each_file, 'r') as json_file:\n",
        "        json_list_CLEARTHIS = list(json_file)\n",
        "    for json_str in json_list_CLEARTHIS:\n",
        "        result_CLEARTHIS = json.loads(json_str)\n",
        "        train_data_CLEARTHIS.append(result_CLEARTHIS)\n",
        "        \n",
        "  train_data_CLEARTHIS = train_data_CLEARTHIS [start : end] # start and end are used to take only 100 traning examples and generate embeddings for them\n",
        "  # we are only taking 100 instances to create embeddings because of the RAM limitations\n",
        "  # creating embeddigns for more than 100 instances at once crashes the Colab session\n",
        "\n",
        "  code_train = []\n",
        "  docstring_train= []\n",
        "\n",
        "  del json_list_CLEARTHIS,result_CLEARTHIS\n",
        "\n",
        "  for i in train_data_CLEARTHIS:\n",
        "\n",
        "    docstring_train.append(i['docstring'])\n",
        "\n",
        "\n",
        "  # we have deleted variables using 'del' and 'reset selective'\n",
        "  # we used these methods to stop Colab session from crashing due to RAM issues\n",
        "  del train_data_CLEARTHIS\n",
        "\n",
        "\n",
        "\n",
        "  tokens = {'input_ids': [], 'attention_mask': []}\n",
        "\n",
        "  # the code below generates embeddings for each docstring from a list of all the dosctrings\n",
        "  for sentence in docstring_train:\n",
        "\n",
        "      # encode each dpcstring and append to dictionary\n",
        "      new_tokens = tokenizer.encode_plus(sentence, max_length=100,\n",
        "                                        truncation=True, padding='max_length',\n",
        "                                        return_tensors='pt')\n",
        "      tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
        "      tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
        "\n",
        "  # reformat list of tensors into single tensor\n",
        "  tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
        "  # attentions masks were used to take care of empty space tokens\n",
        "  tokens['attention_mask'] = torch.stack(tokens['attention_mask'])\n",
        "  outputs_CLEARTHIS = model(**tokens)\n",
        "  embeddings_CLEARTHIS = outputs_CLEARTHIS.last_hidden_state\n",
        "  del outputs_CLEARTHIS \n",
        "  attention_mask = tokens['attention_mask']\n",
        "  mask = attention_mask.unsqueeze(-1).expand(embeddings_CLEARTHIS.size()).float()\n",
        "  masked_embeddings = embeddings_CLEARTHIS * mask\n",
        "  del embeddings_CLEARTHIS\n",
        "  summed_CLEARTHIS = torch.sum(masked_embeddings, 1)\n",
        "  summed_mask_CLEARTHIS = torch.clamp(mask.sum(1), min=1e-9)\n",
        "  train_code_embs = summed_CLEARTHIS / summed_mask_CLEARTHIS\n",
        "  \n",
        "  del summed_CLEARTHIS, summed_mask_CLEARTHIS\n",
        "  %reset_selective -f CLEARTHIS\n",
        "  return train_code_embs\n",
        "\n",
        "  \n",
        "for iteratorr in range(0,2000,100):\n",
        "  start = iteratorr\n",
        "  end = start+100\n",
        "  # we create embeddings for 100 docstrings at a time due to limited RAM\n",
        "  a_CLEARTHIS = find_embs(start,end)\n",
        "  # the final embeddings of 100 docstrings are saved to drive as an array \n",
        "  # tensors are saved to drive as generated embeddings takes time and we had RAM issues along with colab stopping session after an hour\n",
        "  torch.save(a_CLEARTHIS, f'../CodeBERT_embeddings/train_embeddings/doc_tensor_{start}.pt')\n",
        "  %reset_selective -f CLEARTHIS \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings for Code Snippets"
      ],
      "metadata": {
        "id": "VUtOjP6AXyO7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P1EyuYyFdjD"
      },
      "source": [
        "# this code generates embeddings for training code \n",
        "# this function has been defined again as we had to keep code as light and memory efficient as possible, because of which we have used different variables in both the functions\n",
        "\n",
        "def find_embs(start,end):\n",
        "\n",
        "  trian_files = glob.glob(train_path)\n",
        "  train_data_CLEARTHIS = []\n",
        "  for each_file in trian_files:\n",
        "    with open(each_file, 'r') as json_file:\n",
        "        json_list_CLEARTHIS = list(json_file)\n",
        "    for json_str in json_list_CLEARTHIS:\n",
        "        result_CLEARTHIS = json.loads(json_str)\n",
        "        train_data_CLEARTHIS.append(result_CLEARTHIS)\n",
        "        \n",
        "  train_data_CLEARTHIS = train_data_CLEARTHIS [start : end] # start and end are used to take only 100 traning examples and generate embeddings for them\n",
        "  # we are only taking 100 instances to create embeddings because of the RAM limitations\n",
        "  # creating embeddigns for more than 100 instances at once crashes the Colab session\n",
        "\n",
        "  code_train = []\n",
        "\n",
        "  del json_list_CLEARTHIS,result_CLEARTHIS\n",
        "\n",
        "  for i in train_data_CLEARTHIS:\n",
        "\n",
        "    code_train.append(i['docstring'])\n",
        "\n",
        "\n",
        "  # we have deleted variables using 'del' and 'reset selective'\n",
        "  # we used these methods to stop Colab session from crashing due to RAM issues\n",
        "  del train_data_CLEARTHIS\n",
        "\n",
        "\n",
        "\n",
        "  tokens = {'input_ids': [], 'attention_mask': []}\n",
        "\n",
        "  # the code below generates embeddings for each code snippet from a list of all the code snippets\n",
        "  for sentence in docstring_train:\n",
        "\n",
        "      # encode each code snippet and append to dictionary\n",
        "      new_tokens = tokenizer.encode_plus(sentence, max_length=100,\n",
        "                                        truncation=True, padding='max_length',\n",
        "                                        return_tensors='pt')\n",
        "      tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
        "      tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
        "\n",
        "  # reformat list of tensors into single tensor\n",
        "  tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
        "  # attentions masks were used to take care of empty space tokens\n",
        "  tokens['attention_mask'] = torch.stack(tokens['attention_mask'])\n",
        "  outputs_CLEARTHIS = model(**tokens)\n",
        "  embeddings_CLEARTHIS = outputs_CLEARTHIS.last_hidden_state\n",
        "  del outputs_CLEARTHIS \n",
        "  attention_mask = tokens['attention_mask']\n",
        "  mask = attention_mask.unsqueeze(-1).expand(embeddings_CLEARTHIS.size()).float()\n",
        "  masked_embeddings = embeddings_CLEARTHIS * mask\n",
        "  del embeddings_CLEARTHIS\n",
        "  summed_CLEARTHIS = torch.sum(masked_embeddings, 1)\n",
        "  summed_mask_CLEARTHIS = torch.clamp(mask.sum(1), min=1e-9)\n",
        "  train_code_embs = summed_CLEARTHIS / summed_mask_CLEARTHIS\n",
        "  \n",
        "  del summed_CLEARTHIS, summed_mask_CLEARTHIS\n",
        "  %reset_selective -f CLEARTHIS\n",
        "  return train_code_embs\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings for Testing Data"
      ],
      "metadata": {
        "id": "lRknBE_pX6CZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings for Docstrings"
      ],
      "metadata": {
        "id": "vLDWNF4HZT4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this code generates embeddings for testing docstrings \n",
        "# Everything is similar as done for training data, only the variables are different\n",
        "\n",
        "\n",
        "def find_embs(start,end):\n",
        "\n",
        "  test_files = glob.glob(test_path)\n",
        "  test_data_CLEARTHIS = []\n",
        "  for each_file in test_files:\n",
        "    with open(each_file, 'r') as json_file:\n",
        "        json_list_CLEARTHIS = list(json_file)\n",
        "    for json_str in json_list_CLEARTHIS:\n",
        "        result_CLEARTHIS = json.loads(json_str)\n",
        "        test_data_CLEARTHIS.append(result_CLEARTHIS)\n",
        "        \n",
        "  test_data_CLEARTHIS = test_data_CLEARTHIS [start : end] # start and end are used to take only 100 traning examples and generate embeddings for them\n",
        "  # we are only taking 100 instances to create embeddings because of the RAM limitations\n",
        "  # creating embeddigns for more than 100 instances at once crashes the Colab session\n",
        "\n",
        "  docstring_test= []\n",
        "\n",
        "  del json_list_CLEARTHIS,result_CLEARTHIS\n",
        "\n",
        "  for i in test_data_CLEARTHIS:\n",
        "\n",
        "    docstring_test.append(i['docstring'])\n",
        "\n",
        "\n",
        "  # we have deleted variables using 'del' and 'reset selective'\n",
        "  # we used these methods to stop Colab session from crashing due to RAM issues\n",
        "  del test_data_CLEARTHIS\n",
        "\n",
        "\n",
        "\n",
        "  tokens = {'input_ids': [], 'attention_mask': []}\n",
        "\n",
        "  # the code below generates embeddings for each docstring from a list of all the dosctrings\n",
        "  for sentence in docstring_test:\n",
        "\n",
        "      # encode each dpcstring and append to dictionary\n",
        "      new_tokens = tokenizer.encode_plus(sentence, max_length=100,\n",
        "                                        truncation=True, padding='max_length',\n",
        "                                        return_tensors='pt')\n",
        "      tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
        "      tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
        "\n",
        "  # reformat list of tensors into single tensor\n",
        "  tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
        "  # attentions masks were used to take care of empty space tokens\n",
        "  tokens['attention_mask'] = torch.stack(tokens['attention_mask'])\n",
        "  outputs_CLEARTHIS = model(**tokens)\n",
        "  embeddings_CLEARTHIS = outputs_CLEARTHIS.last_hidden_state\n",
        "  del outputs_CLEARTHIS \n",
        "  attention_mask = tokens['attention_mask']\n",
        "  mask = attention_mask.unsqueeze(-1).expand(embeddings_CLEARTHIS.size()).float()\n",
        "  masked_embeddings = embeddings_CLEARTHIS * mask\n",
        "  del embeddings_CLEARTHIS\n",
        "  summed_CLEARTHIS = torch.sum(masked_embeddings, 1)\n",
        "  summed_mask_CLEARTHIS = torch.clamp(mask.sum(1), min=1e-9)\n",
        "  test_code_embs = summed_CLEARTHIS / summed_mask_CLEARTHIS\n",
        "  \n",
        "  del summed_CLEARTHIS, summed_mask_CLEARTHIS\n",
        "  %reset_selective -f CLEARTHIS\n",
        "  return test_code_embs\n",
        "\n",
        "  \n",
        "for iteratorr in range(0,2000,100):\n",
        "  start = iteratorr\n",
        "  end = start+100\n",
        "  # we create embeddings for 100 docstrings at a time due to limited RAM\n",
        "  a_CLEARTHIS = find_embs(start,end)\n",
        "  # the final embeddings of 100 docstrings are saved to drive as an array \n",
        "  # tensors are saved to drive as generated embeddings takes time and we had RAM issues along with colab stopping session after an hour\n",
        "  torch.save(a_CLEARTHIS, f'../CodeBERT_embeddings/test_embeddings/doc_tensor_{start}.pt')\n",
        "  %reset_selective -f CLEARTHIS \n",
        "  "
      ],
      "metadata": {
        "id": "jiSqfMsIXV6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings for Code snippet"
      ],
      "metadata": {
        "id": "ygs13FOfZUvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this code generates embeddings for testing code \n",
        "# Everything is similar as done for training data, only the variables are different\n",
        "\n",
        "\n",
        "def find_embs(start,end):\n",
        "\n",
        "  test_files = glob.glob(test_path)\n",
        "  test_data_CLEARTHIS = []\n",
        "  for each_file in trian_files:\n",
        "    with open(each_file, 'r') as json_file:\n",
        "        json_list_CLEARTHIS = list(json_file)\n",
        "    for json_str in json_list_CLEARTHIS:\n",
        "        result_CLEARTHIS = json.loads(json_str)\n",
        "        test_data_CLEARTHIS.append(result_CLEARTHIS)\n",
        "        \n",
        "  test_data_CLEARTHIS = test_data_CLEARTHIS [start : end] # start and end are used to take only 100 traning examples and generate embeddings for them\n",
        "  # we are only taking 100 instances to create embeddings because of the RAM limitations\n",
        "  # creating embeddigns for more than 100 instances at once crashes the Colab session\n",
        "\n",
        "  code_test= []\n",
        "\n",
        "  del json_list_CLEARTHIS,result_CLEARTHIS\n",
        "\n",
        "  for i in test_data_CLEARTHIS:\n",
        "\n",
        "    code_test.append(i['docstring'])\n",
        "\n",
        "\n",
        "  # we have deleted variables using 'del' and 'reset selective'\n",
        "  # we used these methods to stop Colab session from crashing due to RAM issues\n",
        "  del test_data_CLEARTHIS\n",
        "\n",
        "\n",
        "\n",
        "  tokens = {'input_ids': [], 'attention_mask': []}\n",
        "\n",
        "  # the code below generates embeddings for each docstring from a list of all the dosctrings\n",
        "  for sentence in code_test:\n",
        "\n",
        "      # encode each dpcstring and append to dictionary\n",
        "      new_tokens = tokenizer.encode_plus(sentence, max_length=100,\n",
        "                                        truncation=True, padding='max_length',\n",
        "                                        return_tensors='pt')\n",
        "      tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
        "      tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
        "\n",
        "  # reformat list of tensors into single tensor\n",
        "  tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
        "  # attentions masks were used to take care of empty space tokens\n",
        "  tokens['attention_mask'] = torch.stack(tokens['attention_mask'])\n",
        "  outputs_CLEARTHIS = model(**tokens)\n",
        "  embeddings_CLEARTHIS = outputs_CLEARTHIS.last_hidden_state\n",
        "  del outputs_CLEARTHIS \n",
        "  attention_mask = tokens['attention_mask']\n",
        "  mask = attention_mask.unsqueeze(-1).expand(embeddings_CLEARTHIS.size()).float()\n",
        "  masked_embeddings = embeddings_CLEARTHIS * mask\n",
        "  del embeddings_CLEARTHIS\n",
        "  summed_CLEARTHIS = torch.sum(masked_embeddings, 1)\n",
        "  summed_mask_CLEARTHIS = torch.clamp(mask.sum(1), min=1e-9)\n",
        "  test_code_embs = summed_CLEARTHIS / summed_mask_CLEARTHIS\n",
        "  \n",
        "  del summed_CLEARTHIS, summed_mask_CLEARTHIS\n",
        "  %reset_selective -f CLEARTHIS\n",
        "  return test_code_embs\n",
        "\n",
        "  \n",
        "for iteratorr in range(0,2000,100):\n",
        "  start = iteratorr\n",
        "  end = start+100\n",
        "  # we create embeddings for 100 docstrings at a time due to limited RAM\n",
        "  a_CLEARTHIS = find_embs(start,end)\n",
        "  # the final embeddings of 100 docstrings are saved to drive as an array \n",
        "  # tensors are saved to drive as generated embeddings takes time and we had RAM issues along with colab stopping session after an hour\n",
        "  torch.save(a_CLEARTHIS, f'../CodeBERT_embeddings/test_embeddings/code_tensor_{start}.pt')\n",
        "  %reset_selective -f CLEARTHIS \n",
        "  "
      ],
      "metadata": {
        "id": "ixgFAzF9XV8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaiWntoik5HQ"
      },
      "source": [
        "### Combining all the tensors with 100 instances into one main tensor "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV-tCkahZ3MP"
      },
      "source": [
        "embeddings_code = glob.glob('../CodeBERT_embeddings/train_embeddings/*.pt') # location where tensors with 100 instanes are stored\n",
        "tensor_list=[]\n",
        "for emb in embeddings_code:\n",
        "  x = torch.load(emb)\n",
        "  tensor_list.append(x)\n",
        "train_code_embs = torch.cat(tensor_list, dim=0)\n",
        "torch.save(train_code_embs, f'../CodeBERT_embeddings/train_embeddings/code_doc_main.pt') # change this name according to the experiment for which tensors are being combined\n",
        "# the array saved in this step will be used for code search"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cba759b7Hg2h"
      },
      "source": [
        "# Fine tuning the codeBERT for code search\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDc3MBFyfh-H"
      },
      "source": [
        "!pip install tensorboardX\n",
        "!pip install transformers\n",
        "!pwd\n",
        "import os\n",
        "!git clone https://github.com/microsoft/CodeBERT.git\n",
        "%cd /content/CodeBERT/CodeBERT\n",
        "!rm -rf data data/codesearch\n",
        "!mkdir data data/codesearch\n",
        "%cd data/codesearch\n",
        "!gdown https://drive.google.com/uc?id=1xgSR34XO8xXZg4cZScDYj2eGerBE9iGo  \n",
        "!unzip codesearch_data.zip \n",
        "!rm  codesearch_data.zip\n",
        "%cd ../../codesearch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-wrYiRena8B"
      },
      "source": [
        "# following paths are according to the google colab\n",
        "os.rename('/content/CodeBERT/CodeBERT/data/codesearch/train_valid/python/train.txt', '/content/CodeBERT/CodeBERT/data/codesearch/train_valid/python/train_o.txt')\n",
        "os.rename('/content/CodeBERT/CodeBERT/data/codesearch/train_valid/python/valid.txt', '/content/CodeBERT/CodeBERT/data/codesearch/train_valid/python/valid_o.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVCvDDjljoD0"
      },
      "source": [
        "# changing size of input training data to make Code-BERT finetuning faster. \n",
        "\n",
        "\n",
        "with open('/content/CodeBERT/CodeBERT/data/codesearch/train_valid/python/train_o.txt','r') as f:\n",
        "  file = f.readlines()\n",
        "with open('/content/CodeBERT/CodeBERT/data/codesearch/train_valid/python/train.txt','w')as f:\n",
        "  for i in range(0,1000):\n",
        "    f.write(file[i])\n",
        "with open('/content/CodeBERT/CodeBERT/data/codesearch/train_valid/python/valid_o.txt','r') as f:\n",
        "  file = f.readlines()\n",
        "with open('/content/CodeBERT/CodeBERT/data/codesearch/train_valid/python/valid.txt','w')as f:\n",
        "  for i in range(0,500):\n",
        "    f.write(file[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07R7o4Kee7Tv"
      },
      "source": [
        "!python run_classifier.py \\\n",
        "--model_type roberta \\\n",
        "--task_name codesearch \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--eval_all_checkpoints \\\n",
        "--train_file train.txt \\\n",
        "--dev_file valid.txt \\\n",
        "--max_seq_length 200 \\\n",
        "--per_gpu_train_batch_size 32 \\\n",
        "--per_gpu_eval_batch_size 32 \\\n",
        "--learning_rate 1e-5 \\\n",
        "--num_train_epochs 8 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--overwrite_output_dir \\\n",
        "--data_dir ../data/codesearch/train_valid/python \\\n",
        "--output_dir ./models/python  \\\n",
        "--model_name_or_path microsoft/codebert-base\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}