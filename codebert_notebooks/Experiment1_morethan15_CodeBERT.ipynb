{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experiment1_morethan15_CodeBERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWFu0YmGIwqL"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install nmslib\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import json\n",
        "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
        "from scipy import spatial\n",
        "import heapq\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "from scipy.spatial import distance\n",
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "from sklearn.metrics import ndcg_score\n",
        "import nmslib\n",
        "import glob\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import operator\n",
        "\n",
        "# device = 'cpu' ''' keeping it all on cpu'''#torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaiWntoik5HQ"
      },
      "source": [
        "# Loading the tensors with embeddings for Code and Docstring"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'training_data_length' variable below decides the size of training data for our experiments. This variable can be changed to conduct experiment with different sizes of training data"
      ],
      "metadata": {
        "id": "JcwZ6XEhbiyz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV-tCkahZ3MP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc076bf-a302-487f-a36e-b4e15c96a3c4"
      },
      "source": [
        "training_data_length = 15000\n",
        "test_data_length= 2000\n",
        "\n",
        "train_doc_embds = torch.load('../CodeBERT_embeddings/train_embeddings/doc_embds_20000.pt')\n",
        "train_code_embds = torch.load('../CodeBERT_embeddings/train_embeddings/code_embds_20000.pt')\n",
        "\n",
        "train_code_embds = train_code_embds.detach().numpy() if torch.is_tensor(train_code_embds) else train_code_embds\n",
        "train_code_embds = train_code_embds[:training_data_length]\n",
        "\n",
        "train_doc_embds = train_doc_embds.detach().numpy() if torch.is_tensor(train_doc_embds) else train_doc_embds\n",
        "train_doc_embds = train_doc_embds[:training_data_length]\n",
        "\n",
        "train_code = train_code[:training_data_length]\n",
        "train_doc = train_doc[:training_data_length]\n",
        "\n",
        "\n",
        "\n",
        "test_doc_embds = torch.load('../CodeBERT_embeddings/test_embeddings/doc_embds_2000.pt')\n",
        "test_code_embds = torch.load('../CodeBERT_embeddings/test_embeddings/code_embds_2000.pt')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test_doc_embds = test_doc_embds.detach().numpy()if torch.is_tensor(test_doc_embds) else test_doc_embds\n",
        "test_code_embds = test_code_embds.detach().numpy()if torch.is_tensor(test_code_embds) else test_code_embds\n",
        "\n",
        "\n",
        "test_doc_embds =test_doc_embds[:test_data_length]\n",
        "test_code_embds = test_code_embds[:test_data_length]\n",
        "\n",
        "\n",
        "print('train code examples size --',train_code_embds.shape)\n",
        "print('train doc examples size --',train_doc_embds.shape)\n",
        "print('test code examples size --',test_code_embds.shape)\n",
        "print('test doc examples size --',test_doc_embds.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train code examples size -- (5000, 768)\n",
            "train doc examples size -- (15000, 768)\n",
            "test code examples size -- (2000, 768)\n",
            "test doc examples size -- (2000, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMNE6feGTolu"
      },
      "source": [
        "# finding indexes of docstring with different lengths for experiments \n",
        "\n",
        "less10 = []\n",
        "from10to15 = []\n",
        "great15 = []\n",
        "for indx, i in enumerate(test_doc):\n",
        "  if len(i.split())<11:\n",
        "    less10.append(indx)\n",
        "  elif 10<len(i.split())<16:\n",
        "    from10to15.append(indx)\n",
        "  elif len(i.split())>100:\n",
        "    great15.append(indx)\n",
        "\n",
        "less10 = less10[:400]\n",
        "great15 = great15[:400]\n",
        "\n",
        "\n",
        "print(len(less10))\n",
        "print(len(from10to15))\n",
        "print(len(great15))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjvbhkxCack1",
        "outputId": "d9e6a4d3-095c-436b-e085-a8a56db6943e"
      },
      "source": [
        "# finding train docstring which is most similar  to test docstring \n",
        "# this snippet creates a dictinory which acts as the true label for MRR\n",
        "# we compare test docstring's embeddings with embeddings of all the docstrings which are preesent in training data\n",
        "\n",
        "closest_doc_indexes = {}\n",
        "\n",
        "for index, test_doc_embd in enumerate(test_doc_embds):\n",
        "  test_doc_embd = np.reshape(test_doc_embd,(1,-1))\n",
        "  similarities = cosine_similarity(test_doc_embd,train_doc_embds)[0]\n",
        "  \n",
        "  most_similar_train_doc = np.argmax(similarities)\n",
        "  closest_doc_indexes[index] = most_similar_train_doc\n",
        "\n",
        "print(closest_doc_indexes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 11192, 1: 2021, 2: 8007, 3: 13080, 4: 11006, 5: 13316, 6: 3732, 7: 3732, 8: 5985, 9: 4974, 10: 11005, 11: 13080, 12: 8564, 13: 5799, 14: 7623, 15: 6295, 16: 5828, 17: 8477, 18: 12535, 19: 5192, 20: 11744, 21: 5799, 22: 7520, 23: 13490, 24: 9646, 25: 5044, 26: 4757, 27: 11192, 28: 1474, 29: 5212, 30: 9065, 31: 9411, 32: 11005, 33: 1474, 34: 4635, 35: 8218, 36: 4690, 37: 5189, 38: 11283, 39: 8443, 40: 12609, 41: 4061, 42: 8443, 43: 8443, 44: 4617, 45: 8049, 46: 7520, 47: 5799, 48: 13066, 49: 326, 50: 8451, 51: 1394, 52: 10043, 53: 10172, 54: 6140, 55: 12804, 56: 568, 57: 12035, 58: 3661, 59: 9525, 60: 10049, 61: 8678, 62: 8047, 63: 10358, 64: 13968, 65: 10, 66: 3595, 67: 10049, 68: 3509, 69: 11786, 70: 169, 71: 7973, 72: 1294, 73: 2478, 74: 2793, 75: 11471, 76: 5761, 77: 6647, 78: 14416, 79: 9529, 80: 6567, 81: 14687, 82: 14591, 83: 14795, 84: 14011, 85: 14015, 86: 14889, 87: 14011, 88: 14490, 89: 14013, 90: 9497, 91: 8183, 92: 5145, 93: 13566, 94: 9265, 95: 9265, 96: 14822, 97: 2974, 98: 10513, 99: 4613, 100: 5173, 101: 11275, 102: 12767, 103: 5562, 104: 8589, 105: 9503, 106: 8146, 107: 3381, 108: 12926, 109: 8547, 110: 7332, 111: 10236, 112: 8717, 113: 7974, 114: 7971, 115: 25, 116: 25, 117: 497, 118: 24, 119: 9709, 120: 6513, 121: 3483, 122: 12126, 123: 14478, 124: 12174, 125: 3750, 126: 13262, 127: 8248, 128: 6164, 129: 10130, 130: 10454, 131: 3595, 132: 9900, 133: 10455, 134: 3594, 135: 8484, 136: 3747, 137: 8251, 138: 6129, 139: 5646, 140: 6129, 141: 9860, 142: 8196, 143: 5646, 144: 6067, 145: 11719, 146: 2655, 147: 6307, 148: 6307, 149: 5749, 150: 14011, 151: 14011, 152: 14011, 153: 14011, 154: 14183, 155: 14011, 156: 8462, 157: 14343, 158: 12109, 159: 6567, 160: 3739, 161: 10044, 162: 14636, 163: 3619, 164: 6567, 165: 14478, 166: 9507, 167: 6619, 168: 684, 169: 2653, 170: 10301, 171: 6489, 172: 12615, 173: 9976, 174: 382, 175: 8462, 176: 14106, 177: 12783, 178: 3518, 179: 9119, 180: 747, 181: 2920, 182: 8701, 183: 5369, 184: 6313, 185: 13172, 186: 14950, 187: 10616, 188: 8871, 189: 6632, 190: 12170, 191: 6567, 192: 14304, 193: 14304, 194: 12122, 195: 9186, 196: 14265, 197: 1948, 198: 8365, 199: 9329, 200: 8772, 201: 6565, 202: 14167, 203: 3747, 204: 14908, 205: 14690, 206: 681, 207: 14013, 208: 14795, 209: 14156, 210: 14013, 211: 14013, 212: 14685, 213: 8780, 214: 6567, 215: 1362, 216: 9375, 217: 99, 218: 12343, 219: 12503, 220: 10014, 221: 8666, 222: 8959, 223: 4106, 224: 12130, 225: 9840, 226: 14299, 227: 14172, 228: 14793, 229: 14342, 230: 14172, 231: 14897, 232: 14344, 233: 14793, 234: 3747, 235: 14524, 236: 14524, 237: 12122, 238: 9424, 239: 13971, 240: 6309, 241: 6640, 242: 9099, 243: 8547, 244: 9833, 245: 10697, 246: 9672, 247: 10134, 248: 6530, 249: 3407, 250: 3390, 251: 3337, 252: 6075, 253: 3563, 254: 2255, 255: 8714, 256: 4078, 257: 8547, 258: 12767, 259: 2501, 260: 3522, 261: 9818, 262: 7247, 263: 9628, 264: 4077, 265: 3501, 266: 3501, 267: 12155, 268: 2501, 269: 3739, 270: 13782, 271: 14328, 272: 9930, 273: 178, 274: 155, 275: 8484, 276: 9186, 277: 9186, 278: 6565, 279: 6349, 280: 11763, 281: 8538, 282: 13236, 283: 10856, 284: 8169, 285: 3594, 286: 11836, 287: 9008, 288: 14086, 289: 744, 290: 5749, 291: 5751, 292: 9460, 293: 10521, 294: 10521, 295: 3979, 296: 6874, 297: 12538, 298: 14011, 299: 14018, 300: 14016, 301: 14016, 302: 14013, 303: 14016, 304: 9834, 305: 6564, 306: 14731, 307: 11679, 308: 9119, 309: 14542, 310: 3598, 311: 6573, 312: 9381, 313: 6059, 314: 6582, 315: 11130, 316: 3747, 317: 5711, 318: 4077, 319: 6808, 320: 10, 321: 3784, 322: 8506, 323: 3381, 324: 3747, 325: 14581, 326: 10244, 327: 3747, 328: 12661, 329: 2876, 330: 3645, 331: 9332, 332: 3619, 333: 6945, 334: 6023, 335: 3312, 336: 11836, 337: 9028, 338: 13969, 339: 12767, 340: 9943, 341: 12159, 342: 7968, 343: 10318, 344: 8852, 345: 8852, 346: 3709, 347: 10704, 348: 8289, 349: 14836, 350: 7074, 351: 3628, 352: 8939, 353: 3193, 354: 6288, 355: 9096, 356: 11684, 357: 7293, 358: 10244, 359: 9284, 360: 8882, 361: 1901, 362: 7517, 363: 10788, 364: 3750, 365: 3739, 366: 6560, 367: 3750, 368: 6647, 369: 3617, 370: 6434, 371: 14542, 372: 14686, 373: 6566, 374: 14013, 375: 3747, 376: 14013, 377: 3747, 378: 14017, 379: 14542, 380: 14018, 381: 3747, 382: 14013, 383: 3747, 384: 3747, 385: 3747, 386: 3747, 387: 14478, 388: 6564, 389: 13865, 390: 14520, 391: 8639, 392: 13329, 393: 3668, 394: 10178, 395: 559, 396: 1367, 397: 98, 398: 6140, 399: 12224, 400: 14205, 401: 14908, 402: 3797, 403: 3690, 404: 13874, 405: 13802, 406: 6507, 407: 8927, 408: 13769, 409: 3542, 410: 1135, 411: 14011, 412: 12168, 413: 9605, 414: 14333, 415: 5339, 416: 11641, 417: 3364, 418: 9610, 419: 10178, 420: 10089, 421: 923, 422: 8231, 423: 10, 424: 6666, 425: 14088, 426: 10219, 427: 8880, 428: 435, 429: 3619, 430: 8116, 431: 14522, 432: 14491, 433: 10043, 434: 12165, 435: 3643, 436: 13583, 437: 14014, 438: 6561, 439: 3750, 440: 6912, 441: 10789, 442: 3721, 443: 2701, 444: 5985, 445: 5985, 446: 5985, 447: 14173, 448: 12824, 449: 14156, 450: 14173, 451: 14173, 452: 3747, 453: 14011, 454: 9127, 455: 3747, 456: 14863, 457: 2725, 458: 12292, 459: 4184, 460: 3778, 461: 3033, 462: 14011, 463: 14012, 464: 14011, 465: 14011, 466: 14011, 467: 14011, 468: 14011, 469: 6394, 470: 1135, 471: 14731, 472: 10043, 473: 12129, 474: 12129, 475: 3739, 476: 10043, 477: 6537, 478: 8224, 479: 7970, 480: 405, 481: 405, 482: 8720, 483: 9993, 484: 3379, 485: 3747, 486: 6567, 487: 8908, 488: 14011, 489: 14011, 490: 14011, 491: 14011, 492: 14011, 493: 3747, 494: 14011, 495: 14011, 496: 14011, 497: 6523, 498: 12172, 499: 6912, 500: 4988, 501: 427, 502: 566, 503: 14016, 504: 14011, 505: 8627, 506: 8239, 507: 3747, 508: 3747, 509: 13798, 510: 6570, 511: 8308, 512: 14657, 513: 14303, 514: 14303, 515: 14687, 516: 5790, 517: 3534, 518: 13805, 519: 12133, 520: 14183, 521: 3540, 522: 8772, 523: 3747, 524: 3747, 525: 3747, 526: 3747, 527: 5513, 528: 14345, 529: 14542, 530: 11151, 531: 686, 532: 8774, 533: 3593, 534: 14272, 535: 14011, 536: 14011, 537: 14011, 538: 14011, 539: 1851, 540: 92, 541: 1302, 542: 3619, 543: 7828, 544: 8251, 545: 14478, 546: 3747, 547: 7196, 548: 4508, 549: 112, 550: 3750, 551: 677, 552: 7179, 553: 8908, 554: 14478, 555: 3747, 556: 14478, 557: 14521, 558: 14204, 559: 14083, 560: 14083, 561: 14083, 562: 14083, 563: 14083, 564: 14083, 565: 14083, 566: 14083, 567: 14441, 568: 14441, 569: 14083, 570: 14083, 571: 14441, 572: 4508, 573: 9941, 574: 12534, 575: 12123, 576: 6567, 577: 7125, 578: 7125, 579: 9892, 580: 8486, 581: 2501, 582: 5338, 583: 9464, 584: 14282, 585: 6567, 586: 14171, 587: 14890, 588: 14685, 589: 14068, 590: 14171, 591: 14171, 592: 14793, 593: 3217, 594: 14171, 595: 14687, 596: 10191, 597: 10244, 598: 11893, 599: 14156, 600: 14011, 601: 14542, 602: 14298, 603: 14655, 604: 14522, 605: 14471, 606: 9332, 607: 569, 608: 6081, 609: 7973, 610: 3668, 611: 10178, 612: 559, 613: 10223, 614: 9506, 615: 14948, 616: 14016, 617: 12767, 618: 11679, 619: 8703, 620: 12908, 621: 142, 622: 809, 623: 1837, 624: 11293, 625: 12714, 626: 7423, 627: 4047, 628: 12814, 629: 809, 630: 3594, 631: 3621, 632: 3621, 633: 3381, 634: 506, 635: 6569, 636: 9099, 637: 9005, 638: 14130, 639: 9993, 640: 8854, 641: 14862, 642: 14636, 643: 2653, 644: 12174, 645: 14299, 646: 9529, 647: 6582, 648: 9422, 649: 551, 650: 3750, 651: 6560, 652: 6560, 653: 6560, 654: 1135, 655: 14731, 656: 10043, 657: 12122, 658: 3750, 659: 6564, 660: 10623, 661: 13972, 662: 14011, 663: 3747, 664: 3747, 665: 3747, 666: 14018, 667: 3747, 668: 6534, 669: 3747, 670: 3747, 671: 12522, 672: 13419, 673: 13419, 674: 3661, 675: 9748, 676: 3750, 677: 47, 678: 6140, 679: 12679, 680: 11152, 681: 12679, 682: 6666, 683: 13937, 684: 3747, 685: 1739, 686: 3981, 687: 13972, 688: 3747, 689: 6565, 690: 14985, 691: 7547, 692: 13876, 693: 12130, 694: 3750, 695: 6565, 696: 14636, 697: 14864, 698: 14068, 699: 12883, 700: 6560, 701: 3750, 702: 3360, 703: 7749, 704: 12874, 705: 5605, 706: 12173, 707: 6501, 708: 6582, 709: 6582, 710: 3614, 711: 9243, 712: 3035, 713: 3747, 714: 6690, 715: 12120, 716: 6468, 717: 5006, 718: 3033, 719: 6629, 720: 550, 721: 14949, 722: 6223, 723: 11679, 724: 8240, 725: 591, 726: 591, 727: 10481, 728: 8436, 729: 3422, 730: 14558, 731: 13855, 732: 13658, 733: 6546, 734: 8778, 735: 686, 736: 3643, 737: 14607, 738: 6666, 739: 6544, 740: 3735, 741: 577, 742: 14203, 743: 6530, 744: 14542, 745: 6530, 746: 6530, 747: 6547, 748: 14013, 749: 14011, 750: 14011, 751: 14011, 752: 6559, 753: 10176, 754: 3654, 755: 1135, 756: 13590, 757: 13874, 758: 10022, 759: 9610, 760: 12171, 761: 3613, 762: 6554, 763: 8434, 764: 14985, 765: 8931, 766: 14657, 767: 3760, 768: 10178, 769: 1989, 770: 3605, 771: 10089, 772: 6499, 773: 4756, 774: 3743, 775: 7969, 776: 11221, 777: 6494, 778: 6317, 779: 14098, 780: 376, 781: 12359, 782: 6360, 783: 1416, 784: 6288, 785: 744, 786: 7174, 787: 11671, 788: 3508, 789: 3628, 790: 12173, 791: 3542, 792: 10041, 793: 9403, 794: 3685, 795: 3619, 796: 303, 797: 2585, 798: 8427, 799: 11679, 800: 6528, 801: 7862, 802: 9171, 803: 8013, 804: 7426, 805: 12164, 806: 9973, 807: 4603, 808: 6618, 809: 6579, 810: 12008, 811: 12783, 812: 9691, 813: 13972, 814: 13972, 815: 12140, 816: 3303, 817: 1954, 818: 3510, 819: 8444, 820: 12215, 821: 12215, 822: 8251, 823: 3639, 824: 10478, 825: 1653, 826: 3595, 827: 14636, 828: 12168, 829: 12168, 830: 14807, 831: 3584, 832: 561, 833: 14522, 834: 13170, 835: 10714, 836: 7610, 837: 7610, 838: 6857, 839: 11836, 840: 3457, 841: 13170, 842: 6513, 843: 14950, 844: 14836, 845: 12398, 846: 5593, 847: 3750, 848: 12171, 849: 2892, 850: 2892, 851: 2791, 852: 2791, 853: 14718, 854: 13305, 855: 13158, 856: 13356, 857: 13356, 858: 2932, 859: 13199, 860: 11145, 861: 4, 862: 8427, 863: 8443, 864: 7340, 865: 6747, 866: 8462, 867: 8462, 868: 13558, 869: 2188, 870: 7648, 871: 12219, 872: 12202, 873: 13156, 874: 13469, 875: 5633, 876: 6094, 877: 1894, 878: 10614, 879: 882, 880: 10417, 881: 7664, 882: 7869, 883: 2682, 884: 7124, 885: 9256, 886: 8321, 887: 2220, 888: 13159, 889: 8321, 890: 8562, 891: 13589, 892: 2396, 893: 2048, 894: 13781, 895: 13264, 896: 2510, 897: 2791, 898: 9853, 899: 5334, 900: 4130, 901: 8219, 902: 3637, 903: 12738, 904: 12884, 905: 7542, 906: 4774, 907: 3438, 908: 9471, 909: 2833, 910: 3389, 911: 4866, 912: 1358, 913: 4519, 914: 4280, 915: 11183, 916: 5713, 917: 11139, 918: 6165, 919: 4463, 920: 717, 921: 6245, 922: 6245, 923: 2695, 924: 5478, 925: 4335, 926: 4890, 927: 5410, 928: 8012, 929: 2535, 930: 1689, 931: 4679, 932: 2445, 933: 2802, 934: 2445, 935: 2445, 936: 2802, 937: 2445, 938: 12454, 939: 2445, 940: 2802, 941: 2445, 942: 2445, 943: 13511, 944: 2802, 945: 2445, 946: 2802, 947: 9076, 948: 9980, 949: 5140, 950: 5113, 951: 5715, 952: 4855, 953: 3061, 954: 5575, 955: 4469, 956: 5572, 957: 6240, 958: 6239, 959: 11193, 960: 5181, 961: 349, 962: 5450, 963: 5173, 964: 10340, 965: 5529, 966: 10340, 967: 3077, 968: 12969, 969: 7553, 970: 4486, 971: 5733, 972: 4534, 973: 5413, 974: 7186, 975: 4450, 976: 2494, 977: 11130, 978: 2984, 979: 10859, 980: 4945, 981: 9714, 982: 666, 983: 349, 984: 5110, 985: 4729, 986: 4729, 987: 7648, 988: 4729, 989: 4821, 990: 11037, 991: 4613, 992: 5868, 993: 5845, 994: 2402, 995: 12734, 996: 12734, 997: 11783, 998: 7703, 999: 6238, 1000: 7750, 1001: 5264, 1002: 3091, 1003: 4724, 1004: 4515, 1005: 4456, 1006: 6245, 1007: 5261, 1008: 8196, 1009: 7349, 1010: 6943, 1011: 12338, 1012: 4729, 1013: 4819, 1014: 4486, 1015: 3112, 1016: 3040, 1017: 7905, 1018: 6438, 1019: 3128, 1020: 1539, 1021: 4812, 1022: 2442, 1023: 5152, 1024: 5152, 1025: 7705, 1026: 4704, 1027: 5411, 1028: 4732, 1029: 5259, 1030: 4943, 1031: 5276, 1032: 2419, 1033: 6236, 1034: 11159, 1035: 5021, 1036: 5455, 1037: 4465, 1038: 4870, 1039: 2537, 1040: 5789, 1041: 5243, 1042: 5242, 1043: 4891, 1044: 4826, 1045: 4870, 1046: 4867, 1047: 4763, 1048: 6238, 1049: 5243, 1050: 4820, 1051: 7650, 1052: 13379, 1053: 2902, 1054: 11203, 1055: 11159, 1056: 5570, 1057: 5577, 1058: 3023, 1059: 10500, 1060: 7546, 1061: 5282, 1062: 7826, 1063: 11159, 1064: 5173, 1065: 5218, 1066: 5190, 1067: 4923, 1068: 5757, 1069: 5622, 1070: 5793, 1071: 5793, 1072: 4407, 1073: 5738, 1074: 5741, 1075: 6245, 1076: 5793, 1077: 6245, 1078: 13661, 1079: 12969, 1080: 13661, 1081: 5794, 1082: 13661, 1083: 12969, 1084: 7237, 1085: 7237, 1086: 8353, 1087: 5181, 1088: 5023, 1089: 4763, 1090: 4867, 1091: 10050, 1092: 3019, 1093: 12306, 1094: 4359, 1095: 448, 1096: 5832, 1097: 4165, 1098: 5832, 1099: 7648, 1100: 2659, 1101: 8034, 1102: 4391, 1103: 4729, 1104: 4152, 1105: 5707, 1106: 10800, 1107: 10744, 1108: 10497, 1109: 5733, 1110: 6405, 1111: 5318, 1112: 5735, 1113: 4128, 1114: 2689, 1115: 6405, 1116: 3101, 1117: 3101, 1118: 6405, 1119: 8379, 1120: 4503, 1121: 4338, 1122: 4355, 1123: 4890, 1124: 5434, 1125: 5671, 1126: 2746, 1127: 10941, 1128: 13174, 1129: 4554, 1130: 2728, 1131: 2543, 1132: 4969, 1133: 1892, 1134: 8452, 1135: 5006, 1136: 4870, 1137: 5259, 1138: 4465, 1139: 7716, 1140: 5023, 1141: 10794, 1142: 8539, 1143: 2423, 1144: 2984, 1145: 4890, 1146: 11190, 1147: 5063, 1148: 4870, 1149: 8051, 1150: 4867, 1151: 8317, 1152: 2871, 1153: 5872, 1154: 4238, 1155: 1392, 1156: 5037, 1157: 3340, 1158: 7018, 1159: 2857, 1160: 5282, 1161: 5700, 1162: 11783, 1163: 1159, 1164: 4867, 1165: 4515, 1166: 5229, 1167: 4832, 1168: 5577, 1169: 4129, 1170: 5173, 1171: 10340, 1172: 5577, 1173: 4888, 1174: 4612, 1175: 4196, 1176: 13423, 1177: 5485, 1178: 7687, 1179: 5573, 1180: 10655, 1181: 5577, 1182: 4113, 1183: 4113, 1184: 6241, 1185: 2595, 1186: 1010, 1187: 2089, 1188: 7191, 1189: 9024, 1190: 4466, 1191: 4200, 1192: 5318, 1193: 5318, 1194: 4353, 1195: 7563, 1196: 5453, 1197: 12389, 1198: 4426, 1199: 7921, 1200: 4459, 1201: 2682, 1202: 4391, 1203: 4866, 1204: 4870, 1205: 4726, 1206: 4360, 1207: 4726, 1208: 1159, 1209: 4890, 1210: 4867, 1211: 4730, 1212: 2560, 1213: 2483, 1214: 7125, 1215: 4395, 1216: 6257, 1217: 2858, 1218: 767, 1219: 2890, 1220: 2853, 1221: 3066, 1222: 2685, 1223: 4866, 1224: 7869, 1225: 3042, 1226: 8621, 1227: 7191, 1228: 5449, 1229: 5318, 1230: 5453, 1231: 4353, 1232: 11105, 1233: 11181, 1234: 4276, 1235: 4500, 1236: 5448, 1237: 4364, 1238: 448, 1239: 4506, 1240: 5318, 1241: 4159, 1242: 4273, 1243: 5034, 1244: 5387, 1245: 7921, 1246: 3128, 1247: 3045, 1248: 8958, 1249: 8958, 1250: 6389, 1251: 6389, 1252: 8040, 1253: 2687, 1254: 2802, 1255: 4394, 1256: 12530, 1257: 8621, 1258: 5789, 1259: 5441, 1260: 6595, 1261: 4461, 1262: 4461, 1263: 12200, 1264: 2750, 1265: 4495, 1266: 6245, 1267: 3067, 1268: 2944, 1269: 7301, 1270: 7703, 1271: 5243, 1272: 4866, 1273: 5738, 1274: 5621, 1275: 4426, 1276: 4359, 1277: 5204, 1278: 11190, 1279: 4519, 1280: 13048, 1281: 4355, 1282: 4867, 1283: 4494, 1284: 106, 1285: 4359, 1286: 5573, 1287: 4730, 1288: 1358, 1289: 6405, 1290: 4465, 1291: 3340, 1292: 7139, 1293: 5572, 1294: 600, 1295: 5576, 1296: 5577, 1297: 198, 1298: 3066, 1299: 5577, 1300: 4683, 1301: 4683, 1302: 13449, 1303: 4683, 1304: 2981, 1305: 2676, 1306: 4199, 1307: 4199, 1308: 2685, 1309: 3128, 1310: 4441, 1311: 4162, 1312: 10652, 1313: 4459, 1314: 5410, 1315: 4866, 1316: 4866, 1317: 4485, 1318: 4391, 1319: 5572, 1320: 4486, 1321: 4386, 1322: 12396, 1323: 2695, 1324: 4866, 1325: 4335, 1326: 1605, 1327: 4492, 1328: 11202, 1329: 4866, 1330: 8426, 1331: 8426, 1332: 7916, 1333: 7916, 1334: 2690, 1335: 5719, 1336: 9154, 1337: 5733, 1338: 5313, 1339: 4890, 1340: 5573, 1341: 4335, 1342: 6190, 1343: 5259, 1344: 6257, 1345: 481, 1346: 3389, 1347: 4360, 1348: 12503, 1349: 5577, 1350: 4391, 1351: 11230, 1352: 5522, 1353: 1459, 1354: 2853, 1355: 4166, 1356: 10556, 1357: 2089, 1358: 12396, 1359: 4391, 1360: 5573, 1361: 11247, 1362: 4891, 1363: 4486, 1364: 5243, 1365: 6257, 1366: 5243, 1367: 4355, 1368: 13171, 1369: 5243, 1370: 5243, 1371: 5243, 1372: 4887, 1373: 952, 1374: 952, 1375: 10451, 1376: 4740, 1377: 8020, 1378: 5734, 1379: 5738, 1380: 5738, 1381: 4820, 1382: 13246, 1383: 8136, 1384: 7869, 1385: 4743, 1386: 2628, 1387: 4570, 1388: 5475, 1389: 9218, 1390: 4866, 1391: 2419, 1392: 2671, 1393: 7648, 1394: 7410, 1395: 2595, 1396: 8390, 1397: 2955, 1398: 2853, 1399: 4673, 1400: 2459, 1401: 2485, 1402: 6257, 1403: 5522, 1404: 11877, 1405: 5520, 1406: 4673, 1407: 5280, 1408: 4763, 1409: 5537, 1410: 4945, 1411: 666, 1412: 14952, 1413: 4165, 1414: 4395, 1415: 5309, 1416: 7818, 1417: 4394, 1418: 8475, 1419: 5234, 1420: 5583, 1421: 4736, 1422: 2767, 1423: 5583, 1424: 11243, 1425: 5027, 1426: 2740, 1427: 11040, 1428: 3064, 1429: 9714, 1430: 7869, 1431: 12215, 1432: 11855, 1433: 10289, 1434: 320, 1435: 4441, 1436: 4812, 1437: 4914, 1438: 7126, 1439: 3028, 1440: 5700, 1441: 5789, 1442: 6386, 1443: 4179, 1444: 5628, 1445: 5164, 1446: 4395, 1447: 2663, 1448: 4656, 1449: 5305, 1450: 7125, 1451: 5478, 1452: 5735, 1453: 2853, 1454: 5577, 1455: 4129, 1456: 5577, 1457: 5577, 1458: 5577, 1459: 5453, 1460: 2188, 1461: 4476, 1462: 712, 1463: 4308, 1464: 10001, 1465: 6219, 1466: 12197, 1467: 6187, 1468: 4844, 1469: 11769, 1470: 11762, 1471: 11717, 1472: 11190, 1473: 5180, 1474: 9860, 1475: 4369, 1476: 2853, 1477: 5868, 1478: 7647, 1479: 8619, 1480: 8619, 1481: 5621, 1482: 4790, 1483: 5573, 1484: 13662, 1485: 9537, 1486: 2188, 1487: 4867, 1488: 4338, 1489: 2630, 1490: 11801, 1491: 4862, 1492: 5573, 1493: 3051, 1494: 5577, 1495: 4730, 1496: 4730, 1497: 4730, 1498: 4730, 1499: 5827, 1500: 4337, 1501: 4730, 1502: 7648, 1503: 4360, 1504: 4515, 1505: 4730, 1506: 4337, 1507: 4870, 1508: 8316, 1509: 7819, 1510: 971, 1511: 2188, 1512: 5259, 1513: 5844, 1514: 4196, 1515: 5634, 1516: 3135, 1517: 3135, 1518: 2667, 1519: 5965, 1520: 13246, 1521: 13246, 1522: 5733, 1523: 13391, 1524: 5027, 1525: 11040, 1526: 5789, 1527: 5698, 1528: 2944, 1529: 7648, 1530: 3856, 1531: 5715, 1532: 5242, 1533: 5573, 1534: 4866, 1535: 5140, 1536: 2927, 1537: 5789, 1538: 3797, 1539: 4355, 1540: 481, 1541: 5522, 1542: 4866, 1543: 4763, 1544: 4867, 1545: 11783, 1546: 5789, 1547: 13386, 1548: 12214, 1549: 4355, 1550: 4759, 1551: 140, 1552: 5118, 1553: 11829, 1554: 6252, 1555: 3464, 1556: 1661, 1557: 8898, 1558: 666, 1559: 5448, 1560: 5862, 1561: 8615, 1562: 7750, 1563: 4156, 1564: 2979, 1565: 5868, 1566: 2659, 1567: 4360, 1568: 8283, 1569: 6257, 1570: 5760, 1571: 5767, 1572: 5738, 1573: 5025, 1574: 5016, 1575: 5410, 1576: 7648, 1577: 4475, 1578: 7186, 1579: 5667, 1580: 2188, 1581: 5789, 1582: 4515, 1583: 4334, 1584: 5583, 1585: 13253, 1586: 2962, 1587: 5032, 1588: 4866, 1589: 4866, 1590: 4866, 1591: 5243, 1592: 731, 1593: 4870, 1594: 4867, 1595: 5789, 1596: 4147, 1597: 4147, 1598: 4879, 1599: 4927, 1600: 5908, 1601: 5787, 1602: 5204, 1603: 9066, 1604: 4897, 1605: 4897, 1606: 13439, 1607: 5910, 1608: 2996, 1609: 12776, 1610: 4465, 1611: 7467, 1612: 620, 1613: 2519, 1614: 10794, 1615: 4673, 1616: 10673, 1617: 4898, 1618: 4460, 1619: 11836, 1620: 12734, 1621: 5733, 1622: 5354, 1623: 5868, 1624: 6660, 1625: 7884, 1626: 12122, 1627: 3739, 1628: 10880, 1629: 3605, 1630: 3605, 1631: 588, 1632: 3542, 1633: 3614, 1634: 3747, 1635: 6123, 1636: 1520, 1637: 3556, 1638: 3554, 1639: 8542, 1640: 13669, 1641: 13673, 1642: 786, 1643: 6469, 1644: 6469, 1645: 4276, 1646: 7135, 1647: 7565, 1648: 2463, 1649: 7111, 1650: 679, 1651: 7559, 1652: 7995, 1653: 189, 1654: 188, 1655: 186, 1656: 187, 1657: 13271, 1658: 92, 1659: 3853, 1660: 13254, 1661: 3585, 1662: 4030, 1663: 11750, 1664: 3585, 1665: 620, 1666: 11750, 1667: 4030, 1668: 12776, 1669: 12130, 1670: 12776, 1671: 5680, 1672: 5680, 1673: 6565, 1674: 5881, 1675: 1331, 1676: 1182, 1677: 1524, 1678: 8588, 1679: 619, 1680: 4569, 1681: 10007, 1682: 3508, 1683: 588, 1684: 3579, 1685: 6469, 1686: 13447, 1687: 13368, 1688: 9896, 1689: 4470, 1690: 573, 1691: 3579, 1692: 443, 1693: 1331, 1694: 3619, 1695: 6468, 1696: 3585, 1697: 79, 1698: 3503, 1699: 605, 1700: 9705, 1701: 12173, 1702: 2484, 1703: 58, 1704: 10007, 1705: 10007, 1706: 3527, 1707: 3643, 1708: 6615, 1709: 3542, 1710: 12170, 1711: 12174, 1712: 12125, 1713: 7459, 1714: 11681, 1715: 4114, 1716: 562, 1717: 567, 1718: 3539, 1719: 12171, 1720: 6707, 1721: 1948, 1722: 602, 1723: 5385, 1724: 5293, 1725: 12616, 1726: 12616, 1727: 6, 1728: 5711, 1729: 8588, 1730: 2454, 1731: 7968, 1732: 7562, 1733: 5623, 1734: 6675, 1735: 5132, 1736: 13528, 1737: 1942, 1738: 5731, 1739: 10663, 1740: 4866, 1741: 4620, 1742: 11318, 1743: 11318, 1744: 11318, 1745: 11318, 1746: 11314, 1747: 11538, 1748: 11538, 1749: 11538, 1750: 11538, 1751: 11538, 1752: 11560, 1753: 11747, 1754: 12969, 1755: 10614, 1756: 5152, 1757: 6235, 1758: 9546, 1759: 8640, 1760: 12890, 1761: 1083, 1762: 5016, 1763: 734, 1764: 5513, 1765: 5212, 1766: 13492, 1767: 2974, 1768: 7229, 1769: 1340, 1770: 7229, 1771: 13592, 1772: 13260, 1773: 5470, 1774: 956, 1775: 12840, 1776: 13576, 1777: 13329, 1778: 13260, 1779: 13300, 1780: 13300, 1781: 5173, 1782: 5636, 1783: 13367, 1784: 8153, 1785: 8153, 1786: 13367, 1787: 12969, 1788: 4440, 1789: 13294, 1790: 5731, 1791: 12701, 1792: 4355, 1793: 13250, 1794: 862, 1795: 13250, 1796: 13244, 1797: 13250, 1798: 13250, 1799: 13250, 1800: 966, 1801: 5542, 1802: 4412, 1803: 2280, 1804: 13591, 1805: 13405, 1806: 14497, 1807: 13250, 1808: 8587, 1809: 1353, 1810: 11145, 1811: 8695, 1812: 13672, 1813: 8139, 1814: 5529, 1815: 14759, 1816: 8237, 1817: 13696, 1818: 8573, 1819: 4143, 1820: 8385, 1821: 8385, 1822: 3144, 1823: 13523, 1824: 13146, 1825: 13146, 1826: 1454, 1827: 5594, 1828: 7806, 1829: 6378, 1830: 11835, 1831: 6072, 1832: 8966, 1833: 13533, 1834: 5914, 1835: 871, 1836: 3282, 1837: 3464, 1838: 3046, 1839: 13575, 1840: 2676, 1841: 11376, 1842: 11049, 1843: 11376, 1844: 11376, 1845: 11376, 1846: 6670, 1847: 13138, 1848: 11376, 1849: 11361, 1850: 11376, 1851: 11376, 1852: 11602, 1853: 11376, 1854: 10984, 1855: 11376, 1856: 7117, 1857: 12588, 1858: 13543, 1859: 12840, 1860: 8525, 1861: 8407, 1862: 8407, 1863: 7396, 1864: 13496, 1865: 13527, 1866: 12735, 1867: 13391, 1868: 13492, 1869: 13492, 1870: 2935, 1871: 14370, 1872: 8657, 1873: 3621, 1874: 8922, 1875: 8573, 1876: 13262, 1877: 2811, 1878: 6946, 1879: 13250, 1880: 6118, 1881: 6369, 1882: 10, 1883: 12832, 1884: 8138, 1885: 10426, 1886: 13524, 1887: 12774, 1888: 3071, 1889: 12757, 1890: 4802, 1891: 1725, 1892: 11254, 1893: 4085, 1894: 7396, 1895: 2937, 1896: 9878, 1897: 5187, 1898: 13375, 1899: 14747, 1900: 14162, 1901: 8333, 1902: 8627, 1903: 6546, 1904: 2812, 1905: 2813, 1906: 7459, 1907: 12840, 1908: 2168, 1909: 9711, 1910: 7912, 1911: 2925, 1912: 7468, 1913: 10480, 1914: 8634, 1915: 4740, 1916: 8643, 1917: 13543, 1918: 7981, 1919: 13452, 1920: 13448, 1921: 13448, 1922: 8421, 1923: 1076, 1924: 12233, 1925: 13200, 1926: 13200, 1927: 13200, 1928: 13448, 1929: 4021, 1930: 3518, 1931: 677, 1932: 545, 1933: 8436, 1934: 3296, 1935: 9610, 1936: 14086, 1937: 155, 1938: 545, 1939: 12197, 1940: 13372, 1941: 10466, 1942: 4077, 1943: 6119, 1944: 6204, 1945: 4483, 1946: 2262, 1947: 5594, 1948: 2655, 1949: 2655, 1950: 6169, 1951: 6383, 1952: 6383, 1953: 7939, 1954: 4652, 1955: 5594, 1956: 6940, 1957: 5275, 1958: 11152, 1959: 6491, 1960: 6491, 1961: 3000, 1962: 6832, 1963: 5849, 1964: 9709, 1965: 9993, 1966: 6165, 1967: 10234, 1968: 6537, 1969: 3038, 1970: 264, 1971: 9186, 1972: 593, 1973: 593, 1974: 14731, 1975: 7414, 1976: 8577, 1977: 9031, 1978: 9955, 1979: 8054, 1980: 7625, 1981: 1505, 1982: 1260, 1983: 8869, 1984: 6675, 1985: 9119, 1986: 13237, 1987: 9284, 1988: 8801, 1989: 10242, 1990: 12575, 1991: 3558, 1992: 7408, 1993: 2650, 1994: 2650, 1995: 7968, 1996: 7971, 1997: 589, 1998: 6085, 1999: 529}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8hgUR2p9p8e"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NDCG and Cosine Similarity"
      ],
      "metadata": {
        "id": "iuM8mlI6grvS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28_shOdxHiPN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99c0935f-3ee9-44cf-e140-a4378d1048a0"
      },
      "source": [
        "\n",
        "\n",
        "train_code_embds = train_code_embds.detach().numpy() if torch.is_tensor(train_code_embds) else train_code_embds\n",
        "test_doc_embds = test_doc_embds.detach().numpy()if torch.is_tensor(test_doc_embds) else test_doc_embds\n",
        "\n",
        "ndcga = []\n",
        "best_emb = []\n",
        "best_index = []\n",
        "similarity_scores = []\n",
        "best_similarity_scores=[]\n",
        "best_100_indexes = []\n",
        "best_100_similarities = []\n",
        "\n",
        "for indx, test_doc_embd in enumerate(test_doc_embds):\n",
        "# this if checks the size of input query\n",
        "  if indx in great15:  \n",
        "\n",
        "    test_doc_embd = np.reshape(test_doc_embd,(1,-1))\n",
        "    similarities = cosine_similarity(test_doc_embd,train_code_embds)[0]\n",
        "    similarity_scores.append(similarities)\n",
        "    best_similarity_scores.append(max(similarities))\n",
        "    max_sim_index = np.argmax(similarities)\n",
        "    best_index.append(np.argmax(similarities))\n",
        "    best_emb.append(train_code_embds[np.argmax(similarities)])\n",
        "    best_100_indexes.append((np.argsort(similarities)[-50:]))\n",
        "    best_100_similarities.append([similarities[i] for i in best_100_indexes])\n",
        "    ndcga.append((ndcg_score((test_doc_embd), ([train_code_embds[max_sim_index]]))))\n",
        "  \n",
        "print('Average similarity score',np.sum(best_similarity_scores)/len(best_similarity_scores))\n",
        "print('Average NDCGA is ',sum(ndcga)/len(ndcga))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average similarity score 0.9848800048828125\n",
            "Average NDCGA is  0.9889097737581465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY0g-WzWofc8"
      },
      "source": [
        "## MRR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIhKOFo8cOfI",
        "outputId": "c5855111-d002-46c7-fbb0-716989c322dd"
      },
      "source": [
        "reci_ranks = []\n",
        "for index, top_100 in enumerate(best_100_indexes):\n",
        "  if closest_doc_indexes[index] in top_100:\n",
        "    position_found = list(top_100).index(closest_doc_indexes[index])\n",
        "    reci_rank = 1 / (position_found+1)\n",
        "    reci_ranks.append(reci_rank)\n",
        "  else:\n",
        "    reci_ranks.append(0)\n",
        "print('MRR is ',sum(reci_ranks)/len(reci_ranks))\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of top 50 is 50\n",
            " MRR is --  0.004784113295765302\n"
          ]
        }
      ]
    }
  ]
}